{"cells":[{"cell_type":"markdown","metadata":{"id":"view-in-github"},"source":["<a href=\"https://colab.research.google.com/github/sleighton2022/datasci266-final-project/blob/main/Datasci_266_baseline_summarization.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"]},{"cell_type":"markdown","metadata":{"id":"e-k1jXQHLyFU"},"source":["# Datasci 266 final project - enhancement notebook - BART\n","\n","**Description:**\n","\n","**Structure of the notebook:**\n","\n","**Expected results:**\n","\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"0QDMDB_KqT8m"},"source":["## Setup"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"collapsed":true,"executionInfo":{"elapsed":55159,"status":"ok","timestamp":1721795917952,"user":{"displayName":"Xin C","userId":"03164725939202625018"},"user_tz":240},"id":"Gl5EZTtfK5jL","outputId":"26a7b62a-063f-450a-8096-a023ea4afce7"},"outputs":[{"name":"stdout","output_type":"stream","text":["Requirement already satisfied: bert_score in /usr/local/lib/python3.10/dist-packages (0.3.13)\n","Requirement already satisfied: torch>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from bert_score) (2.3.1+cu121)\n","Requirement already satisfied: pandas>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from bert_score) (2.0.3)\n","Requirement already satisfied: transformers>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from bert_score) (4.42.4)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from bert_score) (1.25.2)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from bert_score) (2.32.3)\n","Requirement already satisfied: tqdm>=4.31.1 in /usr/local/lib/python3.10/dist-packages (from bert_score) (4.66.4)\n","Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from bert_score) (3.7.1)\n","Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from bert_score) (24.1)\n","Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.0.1->bert_score) (2.8.2)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.0.1->bert_score) (2023.4)\n","Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.0.1->bert_score) (2024.1)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.0.0->bert_score) (3.15.4)\n","Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.0.0->bert_score) (4.12.2)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.0.0->bert_score) (1.13.0)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.0.0->bert_score) (3.3)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.0.0->bert_score) (3.1.4)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.0.0->bert_score) (2023.6.0)\n","Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.0.0->bert_score) (12.1.105)\n","Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.0.0->bert_score) (12.1.105)\n","Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.0.0->bert_score) (12.1.105)\n","Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch>=1.0.0->bert_score) (8.9.2.26)\n","Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.0.0->bert_score) (12.1.3.1)\n","Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch>=1.0.0->bert_score) (11.0.2.54)\n","Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch>=1.0.0->bert_score) (10.3.2.106)\n","Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch>=1.0.0->bert_score) (11.4.5.107)\n","Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch>=1.0.0->bert_score) (12.1.0.106)\n","Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /usr/local/lib/python3.10/dist-packages (from torch>=1.0.0->bert_score) (2.20.5)\n","Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.0.0->bert_score) (12.1.105)\n","Requirement already satisfied: triton==2.3.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.0.0->bert_score) (2.3.1)\n","Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.0.0->bert_score) (12.5.82)\n","Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /usr/local/lib/python3.10/dist-packages (from transformers>=3.0.0->bert_score) (0.23.5)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers>=3.0.0->bert_score) (6.0.1)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers>=3.0.0->bert_score) (2024.5.15)\n","Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers>=3.0.0->bert_score) (0.4.3)\n","Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers>=3.0.0->bert_score) (0.19.1)\n","Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->bert_score) (1.2.1)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->bert_score) (0.12.1)\n","Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->bert_score) (4.53.1)\n","Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->bert_score) (1.4.5)\n","Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->bert_score) (9.4.0)\n","Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->bert_score) (3.1.2)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->bert_score) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->bert_score) (3.7)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->bert_score) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->bert_score) (2024.7.4)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas>=1.0.1->bert_score) (1.16.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.0.0->bert_score) (2.1.5)\n","Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.0.0->bert_score) (1.3.0)\n","Requirement already satisfied: sentence-transformers in /usr/local/lib/python3.10/dist-packages (3.0.1)\n","Requirement already satisfied: transformers<5.0.0,>=4.34.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (4.42.4)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (4.66.4)\n","Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (2.3.1+cu121)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (1.25.2)\n","Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (1.2.2)\n","Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (1.11.4)\n","Requirement already satisfied: huggingface-hub>=0.15.1 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (0.23.5)\n","Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (9.4.0)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence-transformers) (3.15.4)\n","Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence-transformers) (2023.6.0)\n","Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence-transformers) (24.1)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence-transformers) (6.0.1)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence-transformers) (2.32.3)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence-transformers) (4.12.2)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (1.13.0)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (3.3)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (3.1.4)\n","Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (12.1.105)\n","Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (12.1.105)\n","Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (12.1.105)\n","Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (8.9.2.26)\n","Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (12.1.3.1)\n","Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (11.0.2.54)\n","Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (10.3.2.106)\n","Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (11.4.5.107)\n","Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (12.1.0.106)\n","Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (2.20.5)\n","Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (12.1.105)\n","Requirement already satisfied: triton==2.3.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (2.3.1)\n","Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.11.0->sentence-transformers) (12.5.82)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.34.0->sentence-transformers) (2024.5.15)\n","Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.34.0->sentence-transformers) (0.4.3)\n","Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.34.0->sentence-transformers) (0.19.1)\n","Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence-transformers) (1.4.2)\n","Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence-transformers) (3.5.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.11.0->sentence-transformers) (2.1.5)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.15.1->sentence-transformers) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.15.1->sentence-transformers) (3.7)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.15.1->sentence-transformers) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.15.1->sentence-transformers) (2024.7.4)\n","Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.11.0->sentence-transformers) (1.3.0)\n"]}],"source":["!pip install -q sentencepiece\n","!pip install -q transformers\n","!pip install -q evaluate\n","!pip install -q datasets\n","!pip install -q pandas\n","!pip install -q rouge_score\n","!pip install -q nltk\n","!pip install bert_score\n","!pip install sentence-transformers\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"KzQBFiaPnr4j"},"outputs":[],"source":["#import evaluate\n","import torch\n","my_device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n","my_device\n","\n","#from transformers import T5Tokenizer, TFT5ForConditionalGeneration\n","#from transformers import BertTokenizer, BertModel\n","\n","import evaluate\n","import numpy as np\n","from transformers import pipeline"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":256,"status":"ok","timestamp":1721795918206,"user":{"displayName":"Xin C","userId":"03164725939202625018"},"user_tz":240},"id":"S5st3b1gVsB-","outputId":"c870e9f2-267a-499b-f6d1-ab472d4f9829"},"outputs":[{"name":"stdout","output_type":"stream","text":["--2024-07-24 04:38:38--  https://raw.githubusercontent.com/sleighton2022/datasci266-final-project/main/summary_utils.py\n","Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n","Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 11185 (11K) [text/plain]\n","Saving to: ‘summary_utils.py.1’\n","\n","\rsummary_utils.py.1    0%[                    ]       0  --.-KB/s               \rsummary_utils.py.1  100%[===================>]  10.92K  --.-KB/s    in 0s      \n","\n","2024-07-24 04:38:38 (113 MB/s) - ‘summary_utils.py.1’ saved [11185/11185]\n","\n"]}],"source":["'''\n","#!pip install git+https://github.com/sleighton2022/datasci266-final-project.git\n","!wget https://raw.githubusercontent.com/sleighton2022/datasci266-final-project/main/summary_utils.py\n","from summary_utils import SummaryEvaluator, DatasetManager, SummaryModel\n","'''"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"executionInfo":{"elapsed":6,"status":"ok","timestamp":1721795918207,"user":{"displayName":"Xin C","userId":"03164725939202625018"},"user_tz":240},"id":"1TY2dvNn3znE","outputId":"11dbf215-bb49-4817-e4e7-7a1abeb2d9df"},"outputs":[{"data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'\\n# Load dataset\\ndataset_manager = DatasetManager(dataset_name=\"xsum\", sample_size=10)\\nsampled_dataset = dataset_manager.load_sampled_dataset()\\n'"]},"execution_count":22,"metadata":{},"output_type":"execute_result"}],"source":["'''\n","# Load dataset\n","dataset_manager = DatasetManager(dataset_name=\"xsum\", sample_size=10)\n","sampled_dataset = dataset_manager.load_sampled_dataset()\n","'''"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"InbcAQuqJxpi"},"outputs":[],"source":["from datasets import load_dataset\n","xsum_dataset = load_dataset(\"xsum\", trust_remote_code=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":273,"status":"ok","timestamp":1721795921803,"user":{"displayName":"Xin C","userId":"03164725939202625018"},"user_tz":240},"id":"OPkCcvj9KAoz","outputId":"5c2011a0-51da-46d7-dd71-441513f94f71"},"outputs":[{"name":"stdout","output_type":"stream","text":["204045\n","11334\n","{'document': 'In Wales, councils are responsible for funding and overseeing schools.\\nBut in England, Mr Osborne\\'s plan will mean local authorities will cease to have a role in providing education.\\nAcademies are directly funded by central government and head teachers have more freedom over admissions and to change the way the school works.\\nIt is a significant development in the continued divergence of schools systems on either side of Offa\\'s Dyke.\\nAnd although the Welsh Government will get extra cash to match the money for English schools to extend the school day, it can spend it on any devolved policy area.\\nMinisters have no plans to follow suit.\\nAt the moment, governing bodies are responsible for setting school hours and they need ministerial permission to make significant changes.\\nThere are already more than 2,000 secondary academies in England and its extension to all state schools is unlikely to shake the Welsh Government\\'s attachment to what they call a \"community, comprehensive model\" for schools.\\nIt rejects claims that freedom given to academies can help drive up standards, and it points to academy-free Scotland as the best performing school system in the UK.\\nEducation Minister Huw Lewis said there was \"very little evidence to suggest\" academies have a positive impact in driving up standards and Wales would not be following the model.\\n\"The Tories have wasted hundreds of millions of pounds on academies and free schools and as the Chancellor finalises his budget plans to slash vital services even further, he is committing them to wasting even more on a failing endeavour.\\n\"We have no plans to introduce the chaos and waste of academies and free schools here in Wales.\"\\nNone of the main parties in May\\'s Assembly election - including the Welsh Conservatives - have said they want to introduce academies in Wales.\\nOwen Hathway, NUT Cymru\\'s policy officer, called the academy plans for England \"scandalous.\".\\n\"There is no evidence that academies work, no evidence that they raise standards, no evidence that they offer better quality education and no evidence that they are what parents and communities want,\" he said.\\n\"Certainly a commitment to comprehensive education is something we would want, and indeed expect, all parties to hold firm to in their manifestos for the forthcoming Welsh election.\"\\nBut the Welsh and English schools systems are still linked by a joint arrangement for teachers\\' pay and conditions.\\nAcademies are not tied to these pay scales so in effect Wednesday\\'s announcement will take all English schools out of the system and raise questions about the viability of an England and Wales pay and conditions structure.\\nThere is already growing momentum for the devolution of teachers\\' pay and conditions.\\nOriginally sceptical, the Welsh Labour Government is now broadly in favour.\\nSome teaching unions remain opposed because of concern that Welsh teachers would end up being paid less than those in England.\\nMr Hathway said teachers were concerned it could lead to regional pay.\\n\"At the same time we do of course recognise that the issue of pay is already becoming a grey area due to the negative changes we see taking place in England,\" he said.\\nBut an even bigger difference between the schools landscape on either side of the border, appears to make separate arrangements for pay increasingly likely in future.', 'summary': 'As Chancellor George Osborne announced all English state schools will become academies, the Welsh Government continues to reject the model here.', 'id': '35821725'}\n"]}],"source":["train_data = xsum_dataset['train']\n","train_data = train_data.shuffle(seed=42)\n","test_data = xsum_dataset['test']\n","test_data = test_data.shuffle(seed=42)\n","print(len(train_data))\n","print(len(test_data))\n","print(train_data[0])\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"UCNR7tXRZZR1"},"outputs":[],"source":["#subsample training dataset\n","sample_size = 500\n","train_dataset = train_data.select(range(sample_size))\n","train_dataset\n","\n","#set test dataset for bart and refine bart models\n","test_dataset = test_data.select(range(200))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"GHlJmGNfKVRj"},"outputs":[],"source":["#global params\n","TOK_MAX_LEN = 1024\n","SUMMARY_MAX_LEN = 80\n","SUMMARY_MIN_LEN = 30\n"]},{"cell_type":"markdown","metadata":{"id":"s3HVfg-1-Egi"},"source":["# 3. Model Enhancement Experiments\n","- two steps: 1. page rank 2. T5/Pegasus\n","- two steps: 1. cluster by topic 2. T5/pegasus\n","- Fine tuning layers：  first layer,  last layer (task specific head)\n","- LoRA\n","\n","BERTSUM: An extension of BERT for summarization that uses a transformer encoder to produce contextual embeddings and a classifier to select sentences.\n","PEGASUS: A transformer-based model specifically designed for summarization tasks. It is trained with a self-supervised objective that masks entire sentences.\n","T5 (Text-to-Text Transfer Transformer): Treats all NLP tasks as text-to-text problems, including summarization. It has been shown to perform well on various summarization benchmarks.\n","BART (Bidirectional and Auto-Regressive Transformers): A transformer model trained to reconstruct corrupted text, making it suitable for both extractive and abstractive summarization.\n","GPT-3: Though primarily a generative language model, it can also perform summarization tasks by generating summaries based on the input text.\n","Example of Using a Transformer Model for Summarization\n"]},{"cell_type":"markdown","metadata":{"id":"h7n_qBk_C1x3"},"source":["# BART Models"]},{"cell_type":"markdown","metadata":{"id":"IraaVbveddrJ"},"source":["first run on pre-trained only model without fine tuning"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xQsNfgw1l0uE"},"outputs":[],"source":["from transformers import BartTokenizer, BartForConditionalGeneration\n","\n","# Load the BART tokenizer and model\n","\n","btokenizer = BartTokenizer.from_pretrained('facebook/bart-large-cnn')\n","bmodel = BartForConditionalGeneration.from_pretrained('facebook/bart-large-cnn')\n","'''\n","btokenizer = BartTokenizer.from_pretrained('facebook/bart-base')\n","bmodel = BartForConditionalGeneration.from_pretrained('facebook/bart-base')\n","\n","btokenizer = BartTokenizer.from_pretrained('facebook/bart-base-xsum')\n","bmodel = BartForConditionalGeneration.from_pretrained('facebook/bart-base-xsum')\n","'''\n","bmodel = bmodel.to(my_device)\n","\n","bart_summaries = []\n","for article in test_dataset['document']:\n","    inputs = btokenizer(article,  return_tensors=\"pt\", max_length=1024, truncation=True)\n","    inputs = inputs.to(my_device)\n","    summary_ids = bmodel.generate(inputs[\"input_ids\"], min_length = 25, max_length= 60,  num_beams=4, do_sample = True)\n","    candidate = btokenizer.batch_decode(summary_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]\n","    bart_summaries.append(candidate)"]},{"cell_type":"markdown","metadata":{"id":"dbsQZL9opS09"},"source":["# fine tuning"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dfBNZvhaD65Y"},"outputs":[],"source":["# Tokenize the dataset\n","def tokenize_function(examples):\n","    inputs = [article for article in examples[\"document\"]]\n","    model_inputs = btokenizer(inputs, max_length=1024, truncation=True, padding=\"max_length\")\n","    with btokenizer.as_target_tokenizer():\n","        labels = btokenizer(examples[\"summary\"], max_length= 80, truncation=True, padding=\"max_length\")\n","    model_inputs[\"labels\"] = labels[\"input_ids\"]\n","    return model_inputs"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"x-tUHoTGD0mv"},"outputs":[],"source":["bart_tok_train = train_dataset.map(tokenize_function, batched=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":296},"executionInfo":{"elapsed":190869,"status":"ok","timestamp":1721796295491,"user":{"displayName":"Xin C","userId":"03164725939202625018"},"user_tz":240},"id":"NGawGxnx5854","outputId":"e15f89e8-3670-47c7-95b4-309b4b6ff1eb"},"outputs":[{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1494: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n","  warnings.warn(\n"]},{"data":{"text/html":["\n","    <div>\n","      \n","      <progress value='150' max='150' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [150/150 02:54, Epoch 3/3]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Epoch</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>1</td>\n","      <td>No log</td>\n","      <td>6.052731</td>\n","    </tr>\n","    <tr>\n","      <td>2</td>\n","      <td>No log</td>\n","      <td>5.166945</td>\n","    </tr>\n","    <tr>\n","      <td>3</td>\n","      <td>No log</td>\n","      <td>4.878640</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n","Non-default generation parameters: {'max_length': 142, 'min_length': 56, 'early_stopping': True, 'num_beams': 4, 'length_penalty': 2.0, 'no_repeat_ngram_size': 3, 'forced_bos_token_id': 0, 'forced_eos_token_id': 2}\n"]},{"data":{"text/html":["\n","    <div>\n","      \n","      <progress value='50' max='50' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [50/50 00:14]\n","    </div>\n","    "],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["{'eval_loss': 4.8786396980285645, 'eval_runtime': 15.2672, 'eval_samples_per_second': 6.55, 'eval_steps_per_second': 3.275, 'epoch': 3.0}\n"]}],"source":["import torch\n","from transformers import BartTokenizer, BartForConditionalGeneration, Trainer, TrainingArguments\n","\n","# Freeze all layers except the first two encoder and decoder layers\n","for param in bmodel.parameters():\n","    param.requires_grad = False\n","\n","for param in bmodel.model.encoder.layers[:2].parameters():\n","    param.requires_grad = True\n","\n","for param in bmodel.model.decoder.layers[:2].parameters():\n","    param.requires_grad = True\n","\n","# Set up the training arguments\n","training_args = TrainingArguments(\n","    output_dir=\"./results\",\n","    evaluation_strategy=\"epoch\",\n","    learning_rate=2e-5,\n","    per_device_train_batch_size=2,\n","    per_device_eval_batch_size=2,\n","    num_train_epochs= 10,\n","    weight_decay=0.01,\n",")\n","\n","# Define the Trainer\n","trainer = Trainer(\n","    model=bmodel,\n","    args=training_args,\n","    train_dataset= bart_tok_train,\n","    eval_dataset= bart_tok_train\n",")\n","\n","# Fine-tune the model\n","trainer.train()\n","\n","metrics = trainer.evaluate()\n","print(metrics)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-CXpempXDX3j"},"outputs":[],"source":["eft_bart_summaries = []\n","for article in test_dataset['document']:\n","    inputs = btokenizer(article,  return_tensors=\"pt\", max_length=1024, truncation=True)\n","    inputs = inputs.to(my_device)\n","    summary_ids = bmodel.generate(inputs[\"input_ids\"], max_length=SUMMARY_MAX_LEN,  num_beams=4, do_sample = True)\n","    candidate = btokenizer.decode(summary_ids[0],skip_special_tokens=True)\n","    eft_bart_summaries.append(candidate)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Pb8Wr0_Yyni8"},"outputs":[],"source":["model_name = 'facebook/bart-large-xsum'\n","bart_xsum_model = BartForConditionalGeneration.from_pretrained(model_name)\n","bart_xsum_tokenizer = BartTokenizer.from_pretrained(model_name)\n","config = bart_xsum_model.config\n","max_position_embeddings_bartsum = config.max_position_embeddings"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/"},"id":"Ux_Y4mFz6hln","outputId":"419c96e7-48be-49f9-acc1-d99e993033b7"},"outputs":[{"name":"stderr","output_type":"stream","text":["Your max_length is set to 80, but your input_length is only 70. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=35)\n","Your max_length is set to 80, but your input_length is only 48. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=24)\n","Your max_length is set to 80, but your input_length is only 52. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=26)\n","Your max_length is set to 80, but your input_length is only 56. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=28)\n","Your max_length is set to 80, but your input_length is only 68. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=34)\n","Your max_length is set to 80, but your input_length is only 71. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=35)\n","Your max_length is set to 80, but your input_length is only 71. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=35)\n","Your max_length is set to 80, but your input_length is only 36. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=18)\n","Your max_length is set to 80, but your input_length is only 49. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=24)\n","Your max_length is set to 80, but your input_length is only 73. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=36)\n","Your max_length is set to 80, but your input_length is only 74. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=37)\n"]}],"source":["summarizer = pipeline(\"summarization\", model= bart_xsum_model, tokenizer = bart_xsum_tokenizer, device=-1)\n","batch_input_ids = bart_xsum_tokenizer(test_dataset['document'], return_tensors=\"pt\", truncation=True, padding=\"max_length\") #, max_length=max_position_embeddings_bartsum,\n","decoded_texts = bart_xsum_tokenizer.batch_decode(batch_input_ids['input_ids'], skip_special_tokens=True)\n","batch_summary = summarizer(\n","    decoded_texts,\n","    max_length= SUMMARY_MAX_LEN,\n","    min_length= SUMMARY_MIN_LEN,\n","    length_penalty= 2.0,\n","    num_beams= 4,\n","    early_stopping= True\n",")\n","xsum_bart_summaries = [x['summary_text'] for x in batch_summary]"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"qLZA0yflpiJI","outputId":"64b2b9f3-94d3-465a-fa76-620794baea48"},"outputs":[{"name":"stdout","output_type":"stream","text":["A woman who was seriously hurt in a fatal hen party motorway crash is now helping other major trauma victims rebuild their lives.\n","Sarah Johnson was one of 21 women in a minibus hit by a lorry on the M62. Her friend Bethany Jones, 18, was killed in the crash in April 2013. Ms Johnson broke her shoulder, back and pelvis and spent time in hospital.\n","Sarah Johnson was one of 21 women heading to Liverpool when their minibus was hit by a lorry on the M62. Her friend Bethany Jones, 18, was killed while Ms Johnson and several others were badly hurt. Ms Johnson, who broke her shoulder, back and pelvis, said the help she received from a charity led her to want to support others.\n","A woman who was seriously injured in a motorway crash which killed her friend has said she wants to help other victims of major trauma to recover.\n","\n","A Tudor manor house has reopened following a £2.2m makeover.\n","Bramall Hall dates back to the reign of William the Conqueror. The estate has been under the ownership of just three families since 1936. A total of 1,400 tickets have sold out for the opening weekend.\n","A total of 1,400 tickets have sold out for the opening weekend at Bramall Hall in Stockport, Greater Manchester. Stained glass windows and ceilings have been restored, while the public will be able to visit the dining room and butler's pantry for the first time. The transformation followed a £1.6m grant from the Heritage Lottery Fund.\n","A Grade II-listed manor house has reopened to the public after a £2.5m revamp and £400,000 of Heritage Lottery funding.\n","\n","Walt Disney World has unveiled a lighthouse memorial for a young boy who was killed by an alligator while on holiday at the Florida theme park.\n","Two-year-old Lane Thomas Graves was killed by an alligator in June 2016. He had been playing in the sand near the resort's Seven Seas Lagoon. The lighthouse has been installed near to where the attack occurred. Disney said they hoped the monument would spread awareness for the\n","Two-year-old Lane Thomas Graves had been playing in the sand near the resort's Seven Seas Lagoon when he was dragged underwater by the creature. His parents and older sister had been visiting the Grand Floridian resort in June 2016 from the state of Nebraska. The lighthouse has been installed near to where the attack occurred.\n","A sculpture of a lighthouse has been installed at Walt Disney World in Florida, a year after a toddler was killed by an alligator in the Magic Kingdom.\n","\n"]}],"source":["#check generated summary vs. reference\n","for i in range(len(test_dataset[:3])):\n","\n","  print(test_dataset['summary'][i])\n","  print(bart_summaries[i])\n","  print(eft_bart_summaries[i])\n","  print(xsum_bart_summaries[i])\n","  print()"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"erqq9NMkqGqj","outputId":"79d87204-3ae7-483f-be4f-9d70127220aa"},"outputs":[{"data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["\"\\n#evaluation\\nsummary_evaluator = SummaryEvaluator()\\nreferences = test_dataset['summary']\\navg_rouge_scores, avg_bleu_score, avg_bert_score, avg_vector_similarity = summary_evaluator.evaluate(references, bart_summaries)\\navg_rouge_scores, avg_bleu_score, avg_bert_score, avg_vector_similarity = summary_evaluator.evaluate(references, eft_bart_summaries)\\n#avg_rouge_scores, avg_bleu_score, avg_bert_score, avg_vector_similarity = summary_evaluator.evaluate(references, xsum_bart_summaries)\\n\""]},"execution_count":35,"metadata":{},"output_type":"execute_result"}],"source":["'''\n","#evaluation\n","summary_evaluator = SummaryEvaluator()\n","references = test_dataset['summary']\n","avg_rouge_scores, avg_bleu_score, avg_bert_score, avg_vector_similarity = summary_evaluator.evaluate(references, bart_summaries)\n","avg_rouge_scores, avg_bleu_score, avg_bert_score, avg_vector_similarity = summary_evaluator.evaluate(references, eft_bart_summaries)\n","#avg_rouge_scores, avg_bleu_score, avg_bert_score, avg_vector_similarity = summary_evaluator.evaluate(references, xsum_bart_summaries)\n","'''"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"k0YSt7bsuoSW","outputId":"8364d657-e601-4520-e6d2-ee3c5b1ec87e"},"outputs":[{"name":"stdout","output_type":"stream","text":["{'bleu': 0.01581709133323252, 'precisions': [0.16147449422862106, 0.02456408298332697, 0.0069217709285621, 0.0022797371597157033], 'brevity_penalty': 1.0, 'length_ratio': 1.7634055592033269, 'translation_length': 8057, 'reference_length': 4569}\n","{'rouge1': 0.2049530846495754, 'rouge2': 0.0363046061986633, 'rougeL': 0.1445025336344798, 'rougeLsum': 0.14425907424312273}\n","\n","{'bleu': 0.008778589167393392, 'precisions': [0.13738719004643826, 0.017390528850441452, 0.003359665849450649, 0.0007398501803384815], 'brevity_penalty': 1.0, 'length_ratio': 2.49792077040928, 'translation_length': 11413, 'reference_length': 4569}\n","{'rouge1': 0.19401733295353862, 'rouge2': 0.02858151556938742, 'rougeL': 0.12854805899940663, 'rougeLsum': 0.12873860534021983}\n","\n","{'bleu': 0.12862279939221086, 'precisions': [0.36580690135935867, 0.1570964247020585, 0.08898463844136381, 0.053522771506422734], 'brevity_penalty': 1.0, 'length_ratio': 1.2558546727949222, 'translation_length': 5738, 'reference_length': 4569}\n","{'rouge1': 0.4054255959513244, 'rouge2': 0.19327759155955582, 'rougeL': 0.3343834002614586, 'rougeLsum': 0.33453386067074503}\n"]}],"source":["bleu = evaluate.load(\"bleu\")\n","rouge = evaluate.load('rouge')\n","\n","references = test_dataset['summary']\n","print(bleu.compute(predictions=bart_summaries, references=references))\n","print(rouge.compute(predictions=bart_summaries, references=references))\n","print()\n","print(bleu.compute(predictions=eft_bart_summaries, references=references))\n","print(rouge.compute(predictions=eft_bart_summaries, references=references))\n","print()\n","print(bleu.compute(predictions=xsum_bart_summaries, references=references))\n","print(rouge.compute(predictions=xsum_bart_summaries, references=references))\n"]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.5"}},"nbformat":4,"nbformat_minor":0}